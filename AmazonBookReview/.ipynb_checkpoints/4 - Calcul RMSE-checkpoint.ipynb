{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from surprise.model_selection import KFold\n",
    "from surprise import accuracy\n",
    "import pandas as pd\n",
    "from surprise import Dataset, evaluate, Reader, accuracy, Trainset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________\n",
      "fold :  0\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inteName' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-14a341902f7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicoDf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdfTemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'userId'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'movieId'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mdfTemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataCV3/df'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0minteName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_fold_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.gzip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gzip'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mdfTemp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inteName' is not defined"
     ]
    }
   ],
   "source": [
    "for fold in range(10):\n",
    "    print(\"______________________________________________________________\")\n",
    "    print(\"fold : \",fold)\n",
    "    print(\"load nb ratings\")\n",
    "    arDico=pd.read_csv('dataCV3/user-nbRating_fold_'+str(fold)+'.csv').to_numpy()\n",
    "    d={}\n",
    "    for i in range(len(arDico)) :\n",
    "        d[arDico[i,0]]=arDico[i,1]\n",
    "    del arDico\n",
    "    \n",
    "    print(\"load testing dataset\")\n",
    "    arTest=pd.read_parquet('dataCV3/dfTest_fold_'+str(fold)+'.gzip').to_numpy()\n",
    "    size=len(arTest)\n",
    "    dicoDf={}\n",
    "    print(\"Repartition Start\")\n",
    "    for j in range(size): \n",
    "        #print(arTest[j,0],d[int(arTest[j,0])])\n",
    "        nb=d[int(arTest[j,0])]//10\n",
    "        if nb in dicoDf :\n",
    "            dicoDf[nb].append([arTest[j]])\n",
    "        else:\n",
    "            dicoDf[nb]=list()\n",
    "            dicoDf[nb].append([arTest[j]])\n",
    "    del d\n",
    "    \n",
    "    print(\"Sauvegarde des intervalles\")\n",
    "    for j in dicoDf:\n",
    "        a=np.concatenate(dicoDf[j],axis=0)\n",
    "        dfTemp = pd.DataFrame({'userId':a[:,0],'movieId':a[:,1],'rating':a[:,2]})\n",
    "        dfTemp.to_parquet('dataCV3/df'+inteName(j)+'_fold_'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "        del dfTemp,a\n",
    "    \n",
    "    print(\"Creation of the large intervalles\")\n",
    "    for i in range(10,50,5):\n",
    "        listDF=list()\n",
    "        for j in range(5):\n",
    "            listDF.append(pd.read_parquet('dataCV3/df'+inteName(i+j)+'_fold_'+str(fold)+'.gzip'))\n",
    "        dfTemp=pd.concat(listDF)\n",
    "        dfTemp.to_parquet('dataCV3/df'+inteName2(i)+'_fold_'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "    \n",
    "    print(\"Creation of the intervalle sup500\")\n",
    "    listDF=list()\n",
    "    for i in range(50,800):\n",
    "        path='dataCV3/df'+inteName(i)+'_fold_'+str(fold)+'.gzip'\n",
    "        if os.path.exists(path):\n",
    "            listDF.append(pd.read_parquet(path))\n",
    "    dfTemp=pd.concat(listDF)\n",
    "    dfTemp.to_parquet('dataCV3/df'+inteName2(50)+'_fold_'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "    del dfTemp,listDF\n",
    "    \n",
    "    print(\"Creation of the intervalle inf20\")\n",
    "    df1=pd.read_parquet('dataCV3/df1-9'+'_fold_'+str(fold)+'.gzip')\n",
    "    df2=pd.read_parquet('dataCV3/df10-19'+'_fold_'+str(fold)+'.gzip')\n",
    "    dfInf20=pd.concat([df1,df2])\n",
    "    dfInf20.to_parquet('dataCV3/dfInf20'+'_fold_'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "    del df1,df2,dfInf20\n",
    "    \n",
    "    print(\"Creation of the intervalle sup20\")\n",
    "    listDF=list()\n",
    "    for i in range(2,10):\n",
    "        listDF.append(pd.read_parquet('dataCV3/df'+inteName2(i)+'_fold_'+str(fold)+'.gzip'))\n",
    "    for i in range(10,51,5):\n",
    "        listDF.append(pd.read_parquet('dataCV3/df'+inteName2(i)+'_fold_'+str(fold)+'.gzip'))\n",
    "    dfSup20=pd.concat(listDF)\n",
    "    dfSup20.to_parquet('dataCV3/dfSup20_fold_'+str(fold)+'.gzip', compression='gzip',index=False)    \n",
    "    del dfSup20,listDF,dicoDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise import BaselineOnly\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNBaseline\n",
    "from surprise import KNNWithZScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8347a4598c39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdicoDfTest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdicoDfTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataCV3/dfTest_fold_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.gzip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'use_pandas_metadata'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         result = self.api.parquet.read_table(path, columns=columns,\n\u001b[1;32m--> 129\u001b[1;33m                                              **kwargs).to_pandas()\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pyarrow\\array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pyarrow\\table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[1;34m(options, table, categories, ignore_metadata)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0m_check_data_column_metadata_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     blocks = _table_to_blocks(options, table, pa.default_memory_pool(),\n\u001b[1;32m--> 610\u001b[1;33m                               categories)\n\u001b[0m\u001b[0;32m    611\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_column_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001b[0m in \u001b[0;36m_table_to_blocks\u001b[1;34m(options, block_table, memory_pool, categories)\u001b[0m\n\u001b[0;32m    878\u001b[0m     \u001b[1;31m# Convert an arrow table to Block from the internal pandas API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m     result = pa.lib.table_to_blocks(options, block_table, memory_pool,\n\u001b[1;32m--> 880\u001b[1;33m                                     categories)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;31m# Defined above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dicoDfTest={}\n",
    "for fold in range(10):\n",
    "    dicoDfTest[fold]=pd.read_parquet('dataCV3/dfTest_fold_'+str(fold)+'.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listTestCV(fold) :    \n",
    "    testInte=list()\n",
    "    reader = Reader()\n",
    "    dfTest=pd.read_parquet('dataCV3/dfTest_fold_'+str(fold)+'.gzip')\n",
    "    arTest=dfTest.to_numpy()\n",
    "    data=Dataset.load_from_df(dfTest,reader) \n",
    "    testInte.append(data.build_full_trainset().build_testset())\n",
    "\n",
    "    df=pd.read_parquet('dataCV3/dfInf20_fold_'+str(fold)+'.gzip')\n",
    "    data=Dataset.load_from_df(df,reader) \n",
    "    testInte.append(data.build_full_trainset().build_testset())\n",
    "\n",
    "    df=pd.read_parquet('dataCV3/dfSup20_fold_'+str(fold)+'.gzip')\n",
    "    data=Dataset.load_from_df(df,reader) \n",
    "    testInte.append(data.build_full_trainset().build_testset())\n",
    "\n",
    "    for j in range(10):\n",
    "        my_file = Path(\"dataCV3/df\"+inteName(j)+'_fold_'+str(fold)+'.gzip')\n",
    "        df=pd.read_parquet(my_file)\n",
    "        data=Dataset.load_from_df(df,reader) \n",
    "        testInte.append(data.build_full_trainset().build_testset())\n",
    "    for j in range(10,51,5):\n",
    "        my_file = Path(\"dataCV3/df\"+inteName(j)+'_fold_'+str(fold)+'.gzip')\n",
    "        df=pd.read_parquet(my_file)\n",
    "        data=Dataset.load_from_df(df,reader) \n",
    "        testInte.append(data.build_full_trainset().build_testset()) \n",
    "    return testInte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inteName(i):\n",
    "    if i==0:\n",
    "        return \"1-9\"\n",
    "    elif i<10:\n",
    "        return str(int(i*10))+\"-\"+str(int(i*10+9))\n",
    "    elif i>=50:\n",
    "        return 'sup500'\n",
    "    else :\n",
    "        return str(int(i*10))+\"-\"+str(int(i*10+49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainsetCV(fold):\n",
    "    dfTrain=pd.read_parquet('dataCV3/dfTrain_fold_'+str(fold)+'.gzip')\n",
    "    reader = Reader()\n",
    "    data=Dataset.load_from_df(dfTrain,reader)\n",
    "    train=data.build_full_trainset()\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoSVD() :\n",
    "    algo=SVD()\n",
    "    return algo\n",
    "    \n",
    "def algoSVDpp() :\n",
    "    algo=SVDpp()\n",
    "    return algo\n",
    "\n",
    "def algoNMF() :\n",
    "    algo=NMF()\n",
    "    return algo\n",
    "\n",
    "def algoBaselineOnly() :\n",
    "    algo=BaselineOnly()\n",
    "    return algo\n",
    "\n",
    "def algoCoClustering() :\n",
    "    algo=CoClustering()\n",
    "    return algo\n",
    "\n",
    "def algoNormalPredictor() :\n",
    "    algo=NormalPredictor()\n",
    "    return algo\n",
    "\n",
    "def algoSlopeOne() :\n",
    "    algo=SlopeOne()\n",
    "    return algo\n",
    "\n",
    "def algoKNNBasic() :\n",
    "    algo=KNNBasic()\n",
    "    return algo\n",
    "\n",
    "def algoKNNWithMeans() :\n",
    "    algo=KNNWithMeans()\n",
    "    return algo\n",
    "\n",
    "def algoKNNBaseline() :\n",
    "    algo=KNNBaseline()\n",
    "    return algo\n",
    "\n",
    "def algoKNNWithZScore() :\n",
    "    algo=KNNWithZScore()\n",
    "    return algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoChoice = {\"SVD\" : algoSVD,\n",
    "              \"SVDpp\" : algoSVDpp,\n",
    "              \"NMF\"  : algoNMF,\n",
    "              \"BaselineOnly\" : algoBaselineOnly,\n",
    "              \"CoClustering\" : algoCoClustering,\n",
    "              \"NormalPredictor\" : algoNormalPredictor,\n",
    "              \"SlopeOne\" : algoSlopeOne,\n",
    "              \"KNNBasic\" : algoKNNBasic,\n",
    "              \"KNNWithMeans\" : algoKNNWithMeans,\n",
    "              \"KNNBaseline\" : algoKNNBaseline,\n",
    "              \"KNNWithZScore\" : algoKNNWithZScore}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoListe=[\"BaselineOnly\", \"SVD\", \"NMF\", \"SlopeOne\", \n",
    "           \"CoClustering\", \"NormalPredictor\", \"SVDpp\",\n",
    "           \"KNNBasic\", \"KNNWithMeans\", \"KNNBaseline\", \"KNNWithZScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajouterResultat(algo,res,listTest,algoName):\n",
    "    rmse=list()\n",
    "    mae=list()\n",
    "    for i in listTest:\n",
    "        predictions = algo.test(i)\n",
    "        rmse.append(accuracy.rmse(predictions, verbose=False))\n",
    "        mae.append(accuracy.mae(predictions, verbose=False))\n",
    "    res[algoName]=rmse+mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listPredictions(ar,algo):\n",
    "    l=list()\n",
    "    t=len(ar)\n",
    "    for i in range(t):\n",
    "        l.append(algo.predict(ar[i,0],ar[i,1],verbose=False)[3])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--==   BaselineOnly   ==--\n",
      "tour  0  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "tour  1  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "tour  2  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "tour  3  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "tour  4  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "tour  5  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "tour  6  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "tour  7  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "tour  8  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "tour  9  de cross val\n",
      "-- Train --\n",
      "Estimating biases using als...\n",
      "-- Test --\n",
      "--==   SVD   ==--\n",
      "tour  0  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  1  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  2  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  3  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  4  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  5  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  6  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  7  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  8  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  9  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "--==   NMF   ==--\n",
      "tour  0  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  1  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  2  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  3  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  4  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  5  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  6  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  7  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  8  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  9  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "--==   SlopeOne   ==--\n",
      "tour  0  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  1  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  2  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  3  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  4  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  5  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  6  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  7  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  8  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  9  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "--==   CoClustering   ==--\n",
      "tour  0  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  1  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  2  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  3  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  4  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  5  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  6  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  7  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  8  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  9  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "--==   NormalPredictor   ==--\n",
      "tour  0  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  1  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  2  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  3  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  4  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  5  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  6  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  7  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  8  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  9  de cross val\n",
      "-- Train --\n",
      "-- Test --\n"
     ]
    }
   ],
   "source": [
    "for name in algoListe[:6] :\n",
    "    print(\"--==  \",name,\"  ==--\")\n",
    "    for fold in range(10): \n",
    "        print(\"tour \",fold,\" de cross val\")\n",
    "        algo=algoChoice[name]()\n",
    "        print(\"-- Train --\")\n",
    "        train=trainsetCV(fold)\n",
    "        algo.fit(train)\n",
    "        print(\"-- Test --\")\n",
    "        ajouterResultat(algo,results,listTestCV(fold),name+str(fold))\n",
    "        dicoDfTest[fold][\"Predicted ratings \"+name]=listPredictions(dicoDfTest[fold].to_numpy(),algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lRMSEname=list()\n",
    "lMAEname=list()\n",
    "for i in range(10):\n",
    "    lRMSEname.append(\"RMSE_\"+inteName(i))\n",
    "    lMAEname.append(\"MAE_\"+inteName(i))\n",
    "for i in range(10,51,5):\n",
    "    lRMSEname.append(\"RMSE_\"+inteName(i))\n",
    "    lMAEname.append(\"MAE_\"+inteName(i))\n",
    "\n",
    "nameColumms=[\"algoName\",\"RMSE_All\",\"RMSE_<20\",\"RMSE_20+\"]+lRMSEname+[\"MAE_All\",\"MAE_<20\",\"MAE_20+\"]+lMAEname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRes=pd.DataFrame(columns =nameColumms)\n",
    "for key in results:\n",
    "    dfRes.loc[len(dfRes)]=[key]+results[key]\n",
    "dfRes.to_csv('data/resultRMSEbyInterval_11.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dicoDfTest :\n",
    "    dicoDfTest[i].to_csv('data/prediction_fold_'+str(i)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat(dicoDfTest.values())\n",
    "df.to_csv('data/prediction_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
