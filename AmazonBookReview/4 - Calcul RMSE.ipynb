{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from surprise.model_selection import KFold\n",
    "from surprise import accuracy\n",
    "import pandas as pd\n",
    "from surprise import Dataset, evaluate, Reader, accuracy, Trainset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inteNameBis(i):\n",
    "    if i==0:\n",
    "        return \"1-9\"\n",
    "    else :\n",
    "        return str(int(i*10))+\"-\"+str(int(i*10+9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________\n",
      "fold :  0\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n",
      "______________________________________________________________\n",
      "fold :  1\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n",
      "______________________________________________________________\n",
      "fold :  2\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n",
      "______________________________________________________________\n",
      "fold :  3\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n",
      "______________________________________________________________\n",
      "fold :  4\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n",
      "______________________________________________________________\n",
      "fold :  5\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n",
      "______________________________________________________________\n",
      "fold :  6\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n",
      "______________________________________________________________\n",
      "fold :  7\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n",
      "______________________________________________________________\n",
      "fold :  8\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n",
      "______________________________________________________________\n",
      "fold :  9\n",
      "load nb ratings\n",
      "load testing dataset\n",
      "Repartition Start\n",
      "Sauvegarde des intervalles\n",
      "Creation of the large intervalles\n",
      "Creation of the intervalle sup500\n",
      "Creation of the intervalle inf20\n",
      "Creation of the intervalle sup20\n"
     ]
    }
   ],
   "source": [
    "for fold in range(10):\n",
    "    print(\"______________________________________________________________\")\n",
    "    print(\"fold : \",fold)\n",
    "    print(\"load nb ratings\")\n",
    "    arDico=pd.read_csv('dataCV3/user-nbRating_fold_'+str(fold)+'.csv').to_numpy()\n",
    "    d={}\n",
    "    for i in range(len(arDico)) :\n",
    "        d[arDico[i,0]]=arDico[i,1]\n",
    "    del arDico\n",
    "    \n",
    "    print(\"load testing dataset\")\n",
    "    arTest=pd.read_parquet('dataCV3/dfTest_fold_'+str(fold)+'.gzip').to_numpy()\n",
    "    size=len(arTest)\n",
    "    dicoDf={}\n",
    "    print(\"Repartition Start\")\n",
    "    for j in range(size): \n",
    "        #print(arTest[j,0],d[int(arTest[j,0])])\n",
    "        nb=d[int(arTest[j,0])]//10\n",
    "        if nb in dicoDf :\n",
    "            dicoDf[nb].append([arTest[j]])\n",
    "        else:\n",
    "            dicoDf[nb]=list()\n",
    "            dicoDf[nb].append([arTest[j]])\n",
    "    del d\n",
    "    \n",
    "    print(\"Sauvegarde des intervalles\")\n",
    "    for j in dicoDf:\n",
    "        a=np.concatenate(dicoDf[j],axis=0)\n",
    "        dfTemp = pd.DataFrame({'userId':a[:,0],'movieId':a[:,1],'rating':a[:,2]})\n",
    "        dfTemp.to_parquet('dataCV3/df'+inteNameBis(j)+'_fold_'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "        del dfTemp,a\n",
    "    \n",
    "    print(\"Creation of the large intervalles\")\n",
    "    for i in range(10,50,5):\n",
    "        listDF=list()\n",
    "        for j in range(5):\n",
    "            listDF.append(pd.read_parquet('dataCV3/df'+inteNameBis(i+j)+'_fold_'+str(fold)+'.gzip'))\n",
    "        dfTemp=pd.concat(listDF)\n",
    "        dfTemp.to_parquet('dataCV3/df'+inteName(i)+'_fold_'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "    \n",
    "    print(\"Creation of the intervalle sup500\")\n",
    "    listDF=list()\n",
    "    for i in range(50,800):\n",
    "        path='dataCV3/df'+inteNameBis(i)+'_fold_'+str(fold)+'.gzip'\n",
    "        if os.path.exists(path):\n",
    "            listDF.append(pd.read_parquet(path))\n",
    "    dfTemp=pd.concat(listDF)\n",
    "    dfTemp.to_parquet('dataCV3/df'+inteName(50)+'_fold_'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "    del dfTemp,listDF\n",
    "    \n",
    "    print(\"Creation of the intervalle inf20\")\n",
    "    df1=pd.read_parquet('dataCV3/df1-9'+'_fold_'+str(fold)+'.gzip')\n",
    "    df2=pd.read_parquet('dataCV3/df10-19'+'_fold_'+str(fold)+'.gzip')\n",
    "    dfInf20=pd.concat([df1,df2])\n",
    "    dfInf20.to_parquet('dataCV3/dfInf20'+'_fold_'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "    del df1,df2,dfInf20\n",
    "    \n",
    "    print(\"Creation of the intervalle sup20\")\n",
    "    listDF=list()\n",
    "    for i in range(2,10):\n",
    "        listDF.append(pd.read_parquet('dataCV3/df'+inteName(i)+'_fold_'+str(fold)+'.gzip'))\n",
    "    for i in range(10,51,5):\n",
    "        listDF.append(pd.read_parquet('dataCV3/df'+inteName(i)+'_fold_'+str(fold)+'.gzip'))\n",
    "    dfSup20=pd.concat(listDF)\n",
    "    dfSup20.to_parquet('dataCV3/dfSup20_fold_'+str(fold)+'.gzip', compression='gzip',index=False)    \n",
    "    del dfSup20,listDF,dicoDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise import BaselineOnly\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNBaseline\n",
    "from surprise import KNNWithZScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicoDfTest={}\n",
    "for fold in range(10):\n",
    "    dicoDfTest[fold]=pd.read_parquet('dataCV3/dfTest_fold_'+str(fold)+'.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listTestCV(fold) :    \n",
    "    testInte=list()\n",
    "    reader = Reader()\n",
    "    dfTest=pd.read_parquet('dataCV3/dfTest_fold_'+str(fold)+'.gzip')\n",
    "    arTest=dfTest.to_numpy()\n",
    "    data=Dataset.load_from_df(dfTest,reader) \n",
    "    testInte.append(data.build_full_trainset().build_testset())\n",
    "\n",
    "    df=pd.read_parquet('dataCV3/dfInf20_fold_'+str(fold)+'.gzip')\n",
    "    data=Dataset.load_from_df(df,reader) \n",
    "    testInte.append(data.build_full_trainset().build_testset())\n",
    "\n",
    "    df=pd.read_parquet('dataCV3/dfSup20_fold_'+str(fold)+'.gzip')\n",
    "    data=Dataset.load_from_df(df,reader) \n",
    "    testInte.append(data.build_full_trainset().build_testset())\n",
    "\n",
    "    for j in range(10):\n",
    "        my_file = Path(\"dataCV3/df\"+inteName(j)+'_fold_'+str(fold)+'.gzip')\n",
    "        df=pd.read_parquet(my_file)\n",
    "        data=Dataset.load_from_df(df,reader) \n",
    "        testInte.append(data.build_full_trainset().build_testset())\n",
    "    for j in range(10,51,5):\n",
    "        my_file = Path(\"dataCV3/df\"+inteName(j)+'_fold_'+str(fold)+'.gzip')\n",
    "        df=pd.read_parquet(my_file)\n",
    "        data=Dataset.load_from_df(df,reader) \n",
    "        testInte.append(data.build_full_trainset().build_testset()) \n",
    "    return testInte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inteName(i):\n",
    "    if i==0:\n",
    "        return \"1-9\"\n",
    "    elif i<10:\n",
    "        return str(int(i*10))+\"-\"+str(int(i*10+9))\n",
    "    elif i>=50:\n",
    "        return 'sup500'\n",
    "    else :\n",
    "        return str(int(i*10))+\"-\"+str(int(i*10+49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainsetCV(fold):\n",
    "    dfTrain=pd.read_parquet('dataCV3/dfTrain_fold_'+str(fold)+'.gzip')\n",
    "    reader = Reader()\n",
    "    data=Dataset.load_from_df(dfTrain,reader)\n",
    "    train=data.build_full_trainset()\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoSVD() :\n",
    "    algo=SVD()\n",
    "    return algo\n",
    "    \n",
    "def algoSVDpp() :\n",
    "    algo=SVDpp()\n",
    "    return algo\n",
    "\n",
    "def algoNMF() :\n",
    "    algo=NMF()\n",
    "    return algo\n",
    "\n",
    "def algoBaselineOnly() :\n",
    "    algo=BaselineOnly()\n",
    "    return algo\n",
    "\n",
    "def algoCoClustering() :\n",
    "    algo=CoClustering()\n",
    "    return algo\n",
    "\n",
    "def algoNormalPredictor() :\n",
    "    algo=NormalPredictor()\n",
    "    return algo\n",
    "\n",
    "def algoSlopeOne() :\n",
    "    algo=SlopeOne()\n",
    "    return algo\n",
    "\n",
    "def algoKNNBasic() :\n",
    "    algo=KNNBasic()\n",
    "    return algo\n",
    "\n",
    "def algoKNNWithMeans() :\n",
    "    algo=KNNWithMeans()\n",
    "    return algo\n",
    "\n",
    "def algoKNNBaseline() :\n",
    "    algo=KNNBaseline()\n",
    "    return algo\n",
    "\n",
    "def algoKNNWithZScore() :\n",
    "    algo=KNNWithZScore()\n",
    "    return algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoChoice = {\"SVD\" : algoSVD,\n",
    "              \"SVDpp\" : algoSVDpp,\n",
    "              \"NMF\"  : algoNMF,\n",
    "              \"BaselineOnly\" : algoBaselineOnly,\n",
    "              \"CoClustering\" : algoCoClustering,\n",
    "              \"NormalPredictor\" : algoNormalPredictor,\n",
    "              \"SlopeOne\" : algoSlopeOne,\n",
    "              \"KNNBasic\" : algoKNNBasic,\n",
    "              \"KNNWithMeans\" : algoKNNWithMeans,\n",
    "              \"KNNBaseline\" : algoKNNBaseline,\n",
    "              \"KNNWithZScore\" : algoKNNWithZScore}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoListe=[\"BaselineOnly\", \"SVD\", \"NMF\", \"SlopeOne\", \n",
    "           \"CoClustering\", \"NormalPredictor\", \"SVDpp\",\n",
    "           \"KNNBasic\", \"KNNWithMeans\", \"KNNBaseline\", \"KNNWithZScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajouterResultat(algo,res,listTest,algoName):\n",
    "    rmse=list()\n",
    "    mae=list()\n",
    "    for i in listTest:\n",
    "        predictions = algo.test(i)\n",
    "        rmse.append(accuracy.rmse(predictions, verbose=False))\n",
    "        mae.append(accuracy.mae(predictions, verbose=False))\n",
    "    res[algoName]=rmse+mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listPredictions(ar,algo):\n",
    "    l=list()\n",
    "    t=len(ar)\n",
    "    for i in range(t):\n",
    "        l.append(algo.predict(ar[i,0],ar[i,1],verbose=False)[3])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--==   NMF   ==--\n",
      "tour  0  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  1  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  2  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  3  de cross val\n",
      "-- Train --\n",
      "-- Test --\n",
      "tour  4  de cross val\n",
      "-- Train --\n",
      "-- Test --\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1f0a1ad5cbf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-- Test --\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0majouterResultat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlistTestCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mdicoDfTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Predicted ratings \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlistPredictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicoDfTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-16fa35cbd588>\u001b[0m in \u001b[0;36majouterResultat\u001b[1;34m(algo, res, listTest, algoName)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmae\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistTest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mrmse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, testset, verbose)\u001b[0m\n\u001b[0;32m    210\u001b[0m                                     \u001b[0mr_ui_trans\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                                     verbose=verbose)\n\u001b[1;32m--> 212\u001b[1;33m                        for (uid, iid, r_ui_trans) in testset]\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    210\u001b[0m                                     \u001b[0mr_ui_trans\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                                     verbose=verbose)\n\u001b[1;32m--> 212\u001b[1;33m                        for (uid, iid, r_ui_trans) in testset]\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name in algoListe[2:6] :\n",
    "    print(\"--==  \",name,\"  ==--\")\n",
    "    for fold in range(5): \n",
    "        print(\"tour \",fold,\" de cross val\")\n",
    "        algo=algoChoice[name]()\n",
    "        print(\"-- Train --\")\n",
    "        train=trainsetCV(fold)\n",
    "        algo.fit(train)\n",
    "        print(\"-- Test --\")\n",
    "        ajouterResultat(algo,results,listTestCV(fold),name+str(fold))\n",
    "        dicoDfTest[fold][\"Predicted ratings \"+name]=listPredictions(dicoDfTest[fold].to_numpy(),algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lRMSEname=list()\n",
    "lMAEname=list()\n",
    "for i in range(10):\n",
    "    lRMSEname.append(\"RMSE_\"+inteName(i))\n",
    "    lMAEname.append(\"MAE_\"+inteName(i))\n",
    "for i in range(10,51,5):\n",
    "    lRMSEname.append(\"RMSE_\"+inteName(i))\n",
    "    lMAEname.append(\"MAE_\"+inteName(i))\n",
    "\n",
    "nameColumms=[\"algoName\",\"RMSE_All\",\"RMSE_<20\",\"RMSE_20+\"]+lRMSEname+[\"MAE_All\",\"MAE_<20\",\"MAE_20+\"]+lMAEname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRes=pd.DataFrame(columns =nameColumms)\n",
    "for key in results:\n",
    "    dfRes.loc[len(dfRes)]=[key]+results[key]\n",
    "dfRes.to_csv('data/resultRMSEbyInterval_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-db26628302ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdicoDfTest\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdicoDfTest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/prediction_2_fold_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 3020\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    300\u001b[0m                                   \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                                   \u001b[0mdate_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                                   quoting=self.quoting)\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcol_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mto_native_types\u001b[1;34m(self, slicer, na_rep, float_format, decimal, quoting, **kwargs)\u001b[0m\n\u001b[0;32m   1995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1996\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mquoting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1997\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1998\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1999\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in dicoDfTest :\n",
    "    dicoDfTest[i].to_csv('data/prediction_2_fold_'+str(i)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicbr\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df=pd.concat(dicoDfTest.values())\n",
    "df.to_csv('data/prediction_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
