{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to create some interval to calculate the RMSE and the MAE for each interval. Firstly we will create some interval like this :\n",
    "1-9, 10-19, 20-29, 30-39..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import numpy as np\n",
    "import os.path\n",
    "import os\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the fold for the cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the trainig and testing we will use the fonction ShuffleSplit from sklearn. The training dataset represent 80% of the foul dataset and the testing 20%. We use a random seed (random_state) for the case where we should execute the code in sevral time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbFold=10\n",
    "kf = ShuffleSplit(n_splits=nbFold, test_size=.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation of the file user-nbRating.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arDico=pd.read_csv('data/user-nbRating.csv').to_numpy()\n",
    "d={}\n",
    "for i in range(len(arDico)):\n",
    "    d[arDico[i,0]]=arDico[i,1]\n",
    "del arDico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48587.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48587.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48587.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48587.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48587.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating\n",
       "0  48587.0      1.0     3.0\n",
       "1  48587.0      4.0     2.0\n",
       "2  48587.0     22.0     3.0\n",
       "3  48587.0     24.0     3.0\n",
       "4  48587.0     25.0     3.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_parquet('data/ratings.gzip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar=df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonction which name each interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the job easier, each interval is named by an integer as follows :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    0 for 1-9\n",
    "    1 for 10-19\n",
    "    2 for 20-29 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following return the name of the interval when we give the integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inteName(i):\n",
    "    if i==0:\n",
    "        return \"1-9\"\n",
    "    else :\n",
    "        return str(int(i*10))+\"-\"+str(int(i*10+9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the creation I use numpy because it's much faster than pandas for this type of problem. I create list of array for each dataset that I want to create, these list are store in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key of this dictionary is an integer which corespond to the interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are list of numpy array, each array is a line in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Firstly we put each line of the dataset in an inteval via the dictionarry. \n",
    "    Secondly we concatenate the list of array in one array. \n",
    "    Thirdly we translate this array in a Dataframe from pandas to save this dataframe in parquet (a compressed file format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<unknown>, line 50)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\vicbr\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3296\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-10-b202d6fb5cc4>\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', 'i=0\\nfor train_index, test_index in kf.split(df): \\n    print(\"--== \",i,\" ==--\")\\n    print(len(test_index))\\n    \\n    listAr=list()\\n    dicoDf={}\\n    print(\"Start of the repartition\")\\n    for j in test_index:\\n        nb=d[ar[j,0]]//10\\n        if nb in dicoDf :\\n            dicoDf[nb].append([ar[j]])\\n        else:\\n            dicoDf[nb]=list()\\n            dicoDf[nb].append([ar[j]])\\n    \\n    \\n    \\n    print(\"Start of the interval saving\")\\n    for j in dicoDf:\\n        a=np.concatenate(dicoDf[j],axis=0)  #concatenation\\n        dfTemp = pd.DataFrame({\\'userId\\':a[:,0],\\'movieId\\':a[:,1],\\'rating\\':a[:,2]}) # Numpy to pandas\\n        listAr.append(a) # add this array to create the foul testing dataset\\n        dfTemp.to_parquet(\\'dataCV/df\\'+inteName(j)+\\'_CV\\'+str(i)+\\'.gzip\\', compression=\\'gzip\\',index=False) # interval saving\\n        del dfTemp,a\\n    del dicoDf\\n    \\n    print(\"creating the testing dataset\")\\n    a=np.concatenate(listAr,axis=0) # concatenation\\n    dfTest= pd.DataFrame({\\'userId\\':a[:,0],\\'movieId\\':a[:,1],\\'rating\\':a[:,2]}) # Numpy to pandas\\n    dfTest.to_parquet(\\'dataCV/dfTest_CV\\'+str(i)+\\'.gzip\\', compression=\\'gzip\\',index=False) # interval saving\\n    del dfTest,a,listAr\\n\\n\\n    print(\"creating the Training dataset\")\\n    \\n    print(\"Start of the repartition\")\\n    dicoDf={}\\n    for j in train_index:\\n        nb=d[ar[j,0]]//10\\n        if nb in dicoDf :\\n            dicoDf[nb].append([ar[j]])\\n        else:\\n            dicoDf[nb]=list()\\n            dicoDf[nb].append([ar[j]])\\n    \\n    \\n    \\n    print(\"Start of the interval saving\")\\n        listTrain=list()\\n\\n    for j in train_index :\\n        listTrain.append([ar[j]])\\n    \\n    a=np.concatenate(listTrain,axis=0)\\n    dfTrain= pd.DataFrame({\\'userId\\':a[:,0],\\'movieId\\':a[:,1],\\'rating\\':a[:,2]})\\n    dfTrain.to_parquet(\\'dataCV/dfTrain_CV\\'+str(i)+\\'.gzip\\', compression=\\'gzip\\',index=False)\\n    del dfTrain,a,listTrain\\n    \\n    i+=1\\n')\n",
      "  File \u001b[0;32m\"C:\\Users\\vicbr\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2352\u001b[0m, in \u001b[0;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[0;32m\"<C:\\Users\\vicbr\\Miniconda3\\lib\\site-packages\\decorator.py:decorator-gen-62>\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35mtime\u001b[0m\n",
      "  File \u001b[0;32m\"C:\\Users\\vicbr\\Miniconda3\\lib\\site-packages\\IPython\\core\\magic.py\"\u001b[0m, line \u001b[0;32m187\u001b[0m, in \u001b[0;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[0;32m\"C:\\Users\\vicbr\\Miniconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\"\u001b[0m, line \u001b[0;32m1255\u001b[0m, in \u001b[0;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\vicbr\\Miniconda3\\lib\\site-packages\\IPython\\core\\compilerop.py\"\u001b[1;36m, line \u001b[1;32m100\u001b[1;36m, in \u001b[1;35mast_parse\u001b[1;36m\u001b[0m\n\u001b[1;33m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<unknown>\"\u001b[1;36m, line \u001b[1;32m50\u001b[0m\n\u001b[1;33m    listTrain=list()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i=0\n",
    "for train_index, test_index in kf.split(df): \n",
    "    print(\"--== \",i,\" ==--\")\n",
    "    print(len(test_index))\n",
    "    \n",
    "    listAr=list()\n",
    "    dicoDf={}\n",
    "    print(\"Start of the repartition\")\n",
    "    for j in test_index:\n",
    "        nb=d[ar[j,0]]//10\n",
    "        if nb in dicoDf :\n",
    "            dicoDf[nb].append([ar[j]])\n",
    "        else:\n",
    "            dicoDf[nb]=list()\n",
    "            dicoDf[nb].append([ar[j]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Start of the interval saving\")\n",
    "    for j in dicoDf:\n",
    "        a=np.concatenate(dicoDf[j],axis=0)  #concatenation\n",
    "        dfTemp = pd.DataFrame({'userId':a[:,0],'movieId':a[:,1],'rating':a[:,2]}) # Numpy to pandas\n",
    "        listAr.append(a) # add this array to create the foul testing dataset\n",
    "        dfTemp.to_parquet('dataCV/df'+inteName(j)+'_CV'+str(i)+'.gzip', compression='gzip',index=False) # interval saving\n",
    "        del dfTemp,a\n",
    "    del dicoDf\n",
    "    \n",
    "    print(\"creating the testing dataset\")\n",
    "    a=np.concatenate(listAr,axis=0) # concatenation\n",
    "    dfTest= pd.DataFrame({'userId':a[:,0],'movieId':a[:,1],'rating':a[:,2]}) # Numpy to pandas\n",
    "    dfTest.to_parquet('dataCV/dfTest_CV'+str(i)+'.gzip', compression='gzip',index=False) # interval saving\n",
    "    del dfTest,a,listAr\n",
    "\n",
    "\n",
    "    print(\"creating the Training dataset\")\n",
    "    \n",
    "    print(\"Start of the repartition\")\n",
    "    dicoDf={}\n",
    "    for j in train_index:\n",
    "        nb=d[ar[j,0]]//10\n",
    "        if nb in dicoDf :\n",
    "            dicoDf[nb].append([ar[j]])\n",
    "        else:\n",
    "            dicoDf[nb]=list()\n",
    "            dicoDf[nb].append([ar[j]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Start of the interval saving\")\n",
    "        listTrain=list()\n",
    "\n",
    "    for j in train_index :\n",
    "        listTrain.append([ar[j]])\n",
    "    \n",
    "    a=np.concatenate(listTrain,axis=0)\n",
    "    dfTrain= pd.DataFrame({'userId':a[:,0],'movieId':a[:,1],'rating':a[:,2]})\n",
    "    dfTrain.to_parquet('dataCV/dfTrain_CV'+str(i)+'.gzip', compression='gzip',index=False)\n",
    "    del dfTrain,a,listTrain\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creation of specific intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the intervals that we created, we can create these intervals :\n",
    "100-149, 150-199,... ,450-499 and >500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inteName2(i):\n",
    "    if i==0:\n",
    "        return \"1-9\"\n",
    "    elif i<10:\n",
    "        return str(int(i*10))+\"-\"+str(int(i*10+9))\n",
    "    elif i>=50:\n",
    "        return 'sup500'\n",
    "    else :\n",
    "        return str(int(i*10))+\"-\"+str(int(i*10+49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(10) :\n",
    "    for i in range(10,50,5) :\n",
    "        listDF=list()\n",
    "        for j in range(5) :\n",
    "            listDF.append(pd.read_parquet('dataCV/df'+inteName(i+j)+'_CV'+str(fold)+'.gzip'))\n",
    "        dfTemp=pd.concat(listDF)\n",
    "        dfTemp.to_parquet('dataCV/df'+inteName2(i)+'_CV'+str(fold)+'.gzip', compression='gzip',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(10) : \n",
    "    listDF=list()\n",
    "    for i in range(50,800) :\n",
    "        path='dataCV/df'+inteName(i)+'_CV'+str(fold)+'.gzip'\n",
    "        if os.path.exists(path):\n",
    "            listDF.append(pd.read_parquet(path))\n",
    "    dfTemp=pd.concat(listDF)\n",
    "    dfTemp.to_parquet('dataCV/df'+inteName2(50)+'_CV'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "    del dfTemp,listDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create the sataset with all the user who rated more than 20 and less than 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(10) : \n",
    "    df1=pd.read_parquet('dataCV/df1-9_CV'+str(fold)+'.gzip')\n",
    "    df2=pd.read_parquet('dataCV/df10-19_CV'+str(fold)+'.gzip')\n",
    "    dfInf20=pd.concat([df1,df2])\n",
    "    dfInf20.to_parquet('dataCV/dfInf20_CV'+str(fold)+'.gzip', compression='gzip',index=False)\n",
    "    del df1,df2,dfInf20\n",
    "    \n",
    "\n",
    "    listDF=list()\n",
    "    for i in range(2,10) :\n",
    "        listDF.append(pd.read_parquet('dataCV/df'+inteName2(i)+'_CV'+str(fold)+'.gzip'))\n",
    "    for i in range(10,51,5):\n",
    "        listDF.append(pd.read_parquet('dataCV/df'+inteName2(i)+'_CV'+str(fold)+'.gzip'))\n",
    "    dfSup20=pd.concat(listDF)\n",
    "    dfSup20.to_parquet('dataCV/dfSup20_CV'+str(fold)+'.gzip', compression='gzip',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
